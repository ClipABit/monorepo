"""
Text Embedding module using CLIP model.

Provides text-to-vector conversion using OpenAI's CLIP model
for semantic search capabilities.
"""

import logging
from typing import Union, List
import numpy as np
import torch
from transformers import CLIPModel, CLIPProcessor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextEmbedder:
    """
    CLIP-based text embedder for semantic search.
    
    Converts text queries into 512-dimensional embeddings using
    OpenAI's CLIP model (ViT-B/32 variant).
    
    Uses the full CLIP model (not just text encoder) to ensure
    compatibility with embeddings generated by text_uploader.py.
    
    Usage:
        embedder = TextEmbedder()
        vector = embedder.embed_single("woman on a train")
    """
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32"):
        """
        Initialize the text embedder.

        Args:
            model_name (str): HuggingFace model identifier for CLIP. Default is "openai/clip-vit-base-patch32".

        Loads the CLIP model and processor at startup (not lazily).
        """
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"TextEmbedder initialized (device: {self.device})")
        self.processor = CLIPProcessor.from_pretrained(self.model_name)
        self.model = CLIPModel.from_pretrained(self.model_name).to(self.device)
        self.model.eval()
        logger.info("CLIP model loaded successfully")
    
    # Model is loaded at initialization; no need for lazy loading
    
    def embed_text(self, text: Union[str, List[str]]) -> np.ndarray:
        """
        Generate embeddings for text input(s).
        
        Args:
            text: Single text string or list of text strings
        
        Returns:
            numpy array of embeddings (512-d, L2-normalized)
            Shape: (512,) for single text, (N, 512) for batch
        """
        self._load_model()
        
        # Handle single string
        if isinstance(text, str):
            text = [text]
        
        # Process inputs using CLIPProcessor
        inputs = self.processor(
            text=text,
            return_tensors="pt",
            padding=True,
            truncation=True
        ).to(self.device)
        
        # Generate embeddings using get_text_features (matches text_uploader.py)
        with torch.no_grad():
            text_features = self.model.get_text_features(**inputs)
            # L2 normalize
            text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
        
        # Convert to numpy
        embeddings = text_features.cpu().numpy()
        
        # Return single vector if single input
        if len(embeddings) == 1:
            return embeddings[0]
        
        return embeddings
    
